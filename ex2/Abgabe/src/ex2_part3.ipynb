{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11717215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, ChiSqSelector, Normalizer, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import getpass\n",
    "from pyspark.ml.feature import VarianceThresholdSelector\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.tuning import TrainValidationSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72158c",
   "metadata": {},
   "source": [
    "## Initialization \n",
    "Set seed and initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd740d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DIC EX 2 - group 36\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark session initialized\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fcb189",
   "metadata": {},
   "source": [
    "## Load in pipeline and data, split data\n",
    "\n",
    "here we reuse the pipe that we created in ex2_part2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808cd1d3-f1ee-4a7f-baad-ff61d4b031f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER      = getpass.getuser() \n",
    "PIPE_PATH = f\"hdfs:///user/{USER}/models/feature_pipe_part2\" \n",
    "\n",
    "data_path = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"       \n",
    "df = spark.read.json(data_path)\n",
    "\n",
    "feat_model = PipelineModel.load(PIPE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca96b4",
   "metadata": {},
   "source": [
    "we split the data in train, validation and test split. Before that we sample the dataset down to 5% due to limited computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae35f7e-1b91-4caa-96d6-737873752dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rows: 2889\n",
      "valid rows: 587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rows: 584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = df.sample(withReplacement=False, fraction=0.05, seed=42) #sample to 5%\n",
    "\n",
    "train_data, temp = df.randomSplit([0.7, 0.3], seed=SEED)\n",
    "\n",
    "valid_data, test_data = temp.randomSplit([0.5, 0.5], seed=SEED)\n",
    "\n",
    "print(f\"train rows: {train_data.count()}\")\n",
    "print(f\"valid rows: {valid_data.count()}\")\n",
    "print(f\"test rows: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96241f2a",
   "metadata": {},
   "source": [
    "as we are going to create our own filtering in the next steps we are deleting the chisquared filtering from the pipe and taking the tf_idf as our input for the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c110ef6e-5aaa-42ec-a30b-099e4668a6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf_out: tf_idf\n"
     ]
    }
   ],
   "source": [
    "stages_no_chi = feat_model.stages\n",
    "stages_no_chi = feat_model.stages\n",
    "if stages_no_chi[-1].__class__.__name__ == \"ChiSqSelectorModel\":\n",
    "    stages_no_chi = stages_no_chi[:-1] \n",
    "\n",
    "idf_out = stages_no_chi[-2].getOutputCol() \n",
    "print(f\"idf_out: {idf_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248b02b",
   "metadata": {},
   "source": [
    "## We create the basic svm models aswell as the extraction models \n",
    "\n",
    "just create the base svm model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf87d03-7d74-4062-bb4b-c8c8587ecddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_svm = LinearSVC(\n",
    "    featuresCol=\"norm\", labelCol=\"label\", predictionCol=\"prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f305e03",
   "metadata": {},
   "source": [
    "here we create two different filtering approaches\n",
    "\n",
    "- chisqselector with 2000 features\n",
    "\n",
    "- Variance ThresholdSelector\n",
    "\n",
    "each of them with a specific normalizer\n",
    "\n",
    "we also create the OneVsRest for our svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcfa6dc9-def5-45d6-a8ae-a1b523bc5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq = ChiSqSelector(\n",
    "        featuresCol=idf_out, outputCol=\"selected\", labelCol=\"label\", numTopFeatures=2000,\n",
    "    )\n",
    "\n",
    "chi_normalizer = Normalizer(\n",
    "    inputCol=\"selected\", outputCol=\"norm\", p=2.0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a8c75d-1619-40aa-83b1-3bc106910012",
   "metadata": {},
   "outputs": [],
   "source": [
    "vts = VarianceThresholdSelector(            \n",
    "      featuresCol=idf_out, \n",
    "      outputCol=\"selected\"\n",
    "    )\n",
    "\n",
    "vts_normalizer = Normalizer(inputCol=\"selected\", outputCol=\"norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ae0df6-1588-4315-bc94-ea743a6e2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr = OneVsRest(classifier=base_svm,\n",
    "                labelCol=\"label\", featuresCol=\"norm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53afe7a",
   "metadata": {},
   "source": [
    "## Pipeline creation and parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5d56569-1439-43f8-9064-86518457ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_pipeline = Pipeline(stages=stages_no_chi + [chisq, chi_normalizer, ovr])\n",
    "vts_pipeline = Pipeline(stages=stages_no_chi + [vts, vts_normalizer, ovr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcdc5f",
   "metadata": {},
   "source": [
    "here we initialize the ParameterGrid with different parameters for the sv,:\n",
    "\n",
    "- max. Iterations: 5, 10\n",
    "- regularization Parameter: 0.1, 1.0, 10.0\n",
    "- standardization: True, False \n",
    "\n",
    "and create the evaluator with the F1 metric \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aae7a88e-f131-4c80-8478-638f004180ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "        .addGrid(base_svm.maxIter, [5,10])\n",
    "        .addGrid(base_svm.regParam, [0.1, 1.0, 10.0])\n",
    "        .addGrid(base_svm.standardization, [True, False])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7adfa",
   "metadata": {},
   "source": [
    "## Reusable pipeline running\n",
    "\n",
    "here we build a function to run both pipelines and safe the best parameters aswell as the F1 score, \n",
    "we then save the results and print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b85dbc-35e9-4264-8ca1-a7ff9c15dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tvs(pipe, param_grid, name):\n",
    "    tvs = TrainValidationSplit(\n",
    "        estimator=pipe,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        trainRatio=0.8,\n",
    "        seed=SEED\n",
    "    )\n",
    "    print(f\"\\n Fitting TVS for {name} â€¦\")\n",
    "    model = tvs.fit(train_data)\n",
    "    val_metrics = model.validationMetrics\n",
    "    best_i     = val_metrics.index(max(val_metrics))\n",
    "    best_val   = val_metrics[best_i]\n",
    "    best_params= tvs.getEstimatorParamMaps()[best_i]\n",
    "    test_pred  = model.bestModel.transform(test_data)\n",
    "    test_f1    = evaluator.evaluate(test_pred)\n",
    "    print(f\" {name}  Best val F1 = {best_val:.4f}  params={best_params}\")\n",
    "    print(f\" {name}  Test F1     = {test_f1:.4f}\")\n",
    "    return best_val, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "965aaf21-e78d-4550-bb42-9dc1285e9724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fitting TVS for VTS â€¦\n",
      "25/05/13 01:58:33 WARN CacheManager: Asked to cache already cached data.\n",
      "25/05/13 01:58:33 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:00:01 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1800:>                                                       (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:00:02 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:01:51 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2502:>                                                       (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:01:58 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:03:44 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3258:>                                                       (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:03:55 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:05:47 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:06:21 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:07:30 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:09:12 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:09:44 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:11:23 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:11:55 WARN BlockManager: Asked to remove block broadcast_14021, which does not exist\n",
      "25/05/13 02:11:57 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:14:13 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8353:>                                                       (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:14:20 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:16:08 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:16:34 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:17:12 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:19:25 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:20:18 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:21:05 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:21:53 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:24:35 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:26:49 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VTS  Best val F1 = 0.4068  params={Param(parent='LinearSVC_6d0c326237d8', name='maxIter', doc='max number of iterations (>= 0).'): 5, Param(parent='LinearSVC_6d0c326237d8', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='LinearSVC_6d0c326237d8', name='standardization', doc='whether to standardize the training features before fitting the model.'): False}\n",
      " VTS  Test F1     = 0.4202\n",
      "\n",
      " Fitting TVS for ChiSq â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:29:05 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:31:19 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:33:27 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:35:45 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:37:48 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:40:15 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:42:50 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:45:48 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:48:28 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:51:51 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:54:48 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:58:29 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 03:00:57 WARN DAGScheduler: Broadcasting large task binary with size 1233.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21002:===========================>                           (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ChiSq  Best val F1 = 0.3612  params={Param(parent='LinearSVC_6d0c326237d8', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LinearSVC_6d0c326237d8', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='LinearSVC_6d0c326237d8', name='standardization', doc='whether to standardize the training features before fitting the model.'): True}\n",
      " ChiSq  Test F1     = 0.3814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "results['var'], results['var_test'] = run_tvs(vts_pipeline, param_grid, \"VTS\")\n",
    "results['chi2'], results['chi2_test'] = run_tvs(chi_pipeline, param_grid, \"ChiSq\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c94af-6c2c-49a2-92c2-0cc21c9b39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
