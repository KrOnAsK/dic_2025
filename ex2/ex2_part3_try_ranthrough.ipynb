{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11717215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, ChiSqSelector, Normalizer, StringIndexer\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import getpass\n",
    "\n",
    "# Set a fixed random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd740d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4054. Attempting port 4055.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4055. Attempting port 4056.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4056. Attempting port 4057.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4057. Attempting port 4058.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4058. Attempting port 4059.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4059. Attempting port 4060.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4060. Attempting port 4061.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4061. Attempting port 4062.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4062. Attempting port 4063.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4063. Attempting port 4064.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4064. Attempting port 4065.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4065. Attempting port 4066.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4066. Attempting port 4067.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4067. Attempting port 4068.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4068. Attempting port 4069.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4069. Attempting port 4070.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4070. Attempting port 4071.\n",
      "25/05/12 11:55:34 WARN Utils: Service 'SparkUI' could not bind on port 4071. Attempting port 4072.\n",
      "25/05/12 11:55:36 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "Spark session initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DIC EX 2 - group 36\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3284ed-8426-446a-a6d4-b47c7fe2184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# DIC Ex 2 · Part 3  —  end-to-end pipeline with One-Vs-Rest SVM + TrainValidationSplit\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover,\n",
    "    CountVectorizer, IDF, ChiSqSelector,\n",
    "    Normalizer, StringIndexer\n",
    ")\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808cd1d3-f1ee-4a7f-baad-ff61d4b031f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pipeline \n",
    "USER      = getpass.getuser() \n",
    "PIPE_PATH = f\"hdfs:///user/{USER}/models/feature_pipe_part2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dae35f7e-1b91-4caa-96d6-737873752dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Loading fitted feature pipeline …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 0 Load & split the data  (edit the path if yours differs)\n",
    "# --------------------------------------------------------------------\n",
    "SEED = 42\n",
    "data_path = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"       \n",
    "df = spark.read.json(data_path)\n",
    "\n",
    "print(\"→ Loading fitted feature pipeline …\")\n",
    "feat_model = PipelineModel.load(PIPE_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c110ef6e-5aaa-42ec-a30b-099e4668a6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️  train rows: 55401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 valid rows: 11838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 test rows       : 11590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, category: string, helpful: array<bigint>, overall: double, reviewText: string, reviewTime: string, reviewerID: string, reviewerName: string, summary: string, unixReviewTime: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Splitting data …\")\n",
    "train_data, temp = df.randomSplit([0.7, 0.3], seed=SEED)\n",
    "\n",
    "valid_data, test_data = temp.randomSplit([0.5, 0.5], seed=SEED)\n",
    "\n",
    "print(f\"⚙️  train rows: {train_data.count()}\")\n",
    "print(f\"🔍 valid rows: {valid_data.count()}\")\n",
    "print(f\"🧪 test rows       : {test_data.count()}\")\n",
    "\n",
    "train_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf87d03-7d74-4062-bb4b-c8c8587ecddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf_idf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages_no_chi = feat_model.stages\n",
    "stages_no_chi = feat_model.stages\n",
    "if stages_no_chi[-1].__class__.__name__ == \"ChiSqSelectorModel\":\n",
    "    stages_no_chi = stages_no_chi[:-1] \n",
    "\n",
    "idf_out = stages_no_chi[-2].getOutputCol() \n",
    "idf_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ae0df6-1588-4315-bc94-ea743a6e2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chisq = ChiSqSelector(\n",
    "    featuresCol=idf_out, outputCol=\"selected\", labelCol=\"label\"\n",
    ")\n",
    "\n",
    "normalizer = Normalizer(\n",
    "    inputCol=\"selected\", outputCol=\"norm\", p=2.0\n",
    ")\n",
    "\n",
    "base_svm = LinearSVC(\n",
    "    featuresCol=\"norm\", labelCol=\"label\", predictionCol=\"prediction\",\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "ovr = OneVsRest(classifier=base_svm,\n",
    "                labelCol=\"label\", featuresCol=\"norm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d56569-1439-43f8-9064-86518457ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 2 Full pipeline\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "pipeline = Pipeline(stages=stages_no_chi + [chisq, normalizer, ovr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aae7a88e-f131-4c80-8478-638f004180ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 3 Hyper-parameter grid (6 points)\n",
    "# --------------------------------------------------------------------\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(chisq.numTopFeatures, [2000, 500])\n",
    "    .addGrid(base_svm.regParam, [0.1, 1.0, 10.0])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8,           # 80 % (of 5 %) used to fit per grid point\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "965aaf21-e78d-4550-bb42-9dc1285e9724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳  Fitting TVS grid …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 11:57:18 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/12 11:57:18 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 11:57:19 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "25/05/12 11:57:19 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 12:09:45 WARN DAGScheduler: Broadcasting large task binary with size 1237.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 12:21:04 WARN DAGScheduler: Broadcasting large task binary with size 1237.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 12:30:38 WARN DAGScheduler: Broadcasting large task binary with size 1237.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 13:03:44 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "25/05/12 13:03:47 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 13:04:22 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 13:05:26 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "25/05/12 13:05:29 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 13:06:37 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/12 13:09:33 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 4 Fit & evaluate\n",
    "# --------------------------------------------------------------------\n",
    "print(\"⏳  Fitting TVS grid …\")\n",
    "tvs_model = tvs.fit(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb6e4543-51b3-4fef-b2ca-88846d449d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏅  Best validation F1 = 0.5893\n",
      "Best params           : {Param(parent='ChiSqSelector_b1c3f09f47b9', name='numTopFeatures', doc='Number of features that selector will select, ordered by ascending p-value. If the number of features is < numTopFeatures, then this will select all features.'): 2000, Param(parent='LinearSVC_5a196dc8785c', name='regParam', doc='regularization parameter (>= 0).'): 0.1}\n",
      "\n",
      "🧪  Evaluating on test set …\n",
      "25/05/12 13:24:29 WARN DAGScheduler: Broadcasting large task binary with size 1244.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31201:>                                                      (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1               : 0.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "metrics = tvs_model.validationMetrics          \n",
    "\n",
    "best_idx   = metrics.index(max(metrics))       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_val_f = metrics[best_idx]\n",
    "best_pars  = tvs_model.getEstimatorParamMaps()[best_idx]\n",
    "\n",
    "print(f\"🏅  Best validation F1 = {best_val_f:.4f}\")\n",
    "print(\"Best params           :\", best_pars)\n",
    "\n",
    "print(\"\\n🧪  Evaluating on test set …\")\n",
    "test_pred = tvs_model.bestModel.transform(test_data)\n",
    "test_f1   = evaluator.evaluate(test_pred)\n",
    "print(\"Test F1               :\", round(test_f1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a8c94af-6c2c-49a2-92c2-0cc21c9b39a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainValidationSplitModel_429d1a4d3188"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvs_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
