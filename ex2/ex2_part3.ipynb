{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11717215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, ChiSqSelector, Normalizer, StringIndexer\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import getpass\n",
    "from pyspark.ml.feature import VarianceThresholdSelector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover,\n",
    "    CountVectorizer, IDF, ChiSqSelector,\n",
    "    Normalizer, StringIndexer\n",
    ")\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72158c",
   "metadata": {},
   "source": [
    "Set seed and initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd740d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4054. Attempting port 4055.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4055. Attempting port 4056.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4056. Attempting port 4057.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4057. Attempting port 4058.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4058. Attempting port 4059.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4059. Attempting port 4060.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4060. Attempting port 4061.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4061. Attempting port 4062.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4062. Attempting port 4063.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4063. Attempting port 4064.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4064. Attempting port 4065.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4065. Attempting port 4066.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4066. Attempting port 4067.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4067. Attempting port 4068.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4068. Attempting port 4069.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4069. Attempting port 4070.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4070. Attempting port 4071.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4071. Attempting port 4072.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4072. Attempting port 4073.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4073. Attempting port 4074.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4074. Attempting port 4075.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4075. Attempting port 4076.\n",
      "25/05/13 01:45:37 WARN Utils: Service 'SparkUI' could not bind on port 4076. Attempting port 4077.\n",
      "25/05/13 01:45:40 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "Spark session initialized\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DIC EX 2 - group 36\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Spark session initialized\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fcb189",
   "metadata": {},
   "source": [
    "here we reuse the pipe that we created in ex2_part2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "808cd1d3-f1ee-4a7f-baad-ff61d4b031f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "USER      = getpass.getuser() \n",
    "PIPE_PATH = f\"hdfs:///user/{USER}/models/feature_pipe_part2\" \n",
    "\n",
    "data_path = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"       \n",
    "df = spark.read.json(data_path)\n",
    "\n",
    "feat_model = PipelineModel.load(PIPE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca96b4",
   "metadata": {},
   "source": [
    "we split the data in train, validation and test split. Before that we sample the dataset down to 5% due to limited computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae35f7e-1b91-4caa-96d6-737873752dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rows: 2889\n",
      "valid rows: 587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rows: 584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = df.sample(withReplacement=False, fraction=0.05, seed=42) #sample to 5%\n",
    "\n",
    "train_data, temp = df.randomSplit([0.7, 0.3], seed=SEED)\n",
    "\n",
    "valid_data, test_data = temp.randomSplit([0.5, 0.5], seed=SEED)\n",
    "\n",
    "print(f\"train rows: {train_data.count()}\")\n",
    "print(f\"valid rows: {valid_data.count()}\")\n",
    "print(f\"test rows: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96241f2a",
   "metadata": {},
   "source": [
    "as we are going to create our own filtering in the next steps we are deleting the chisquared filtering from the pipe and taking the tf_idf as our input for the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c110ef6e-5aaa-42ec-a30b-099e4668a6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf_out: tf_idf\n"
     ]
    }
   ],
   "source": [
    "stages_no_chi = feat_model.stages\n",
    "stages_no_chi = feat_model.stages\n",
    "if stages_no_chi[-1].__class__.__name__ == \"ChiSqSelectorModel\":\n",
    "    stages_no_chi = stages_no_chi[:-1] \n",
    "\n",
    "idf_out = stages_no_chi[-2].getOutputCol() \n",
    "print(f\"idf_out: {idf_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248b02b",
   "metadata": {},
   "source": [
    "just create the base svm model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf87d03-7d74-4062-bb4b-c8c8587ecddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearSVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m base_svm = \u001b[43mLinearSVC\u001b[49m(\n\u001b[32m      2\u001b[39m     featuresCol=\u001b[33m\"\u001b[39m\u001b[33mnorm\u001b[39m\u001b[33m\"\u001b[39m, labelCol=\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m, predictionCol=\u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'LinearSVC' is not defined"
     ]
    }
   ],
   "source": [
    "base_svm = LinearSVC(\n",
    "    featuresCol=\"norm\", labelCol=\"label\", predictionCol=\"prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f305e03",
   "metadata": {},
   "source": [
    "here we create two different filtering approaches\n",
    "\n",
    "- chisqselector with 2000 features\n",
    "\n",
    "- Variance ThresholdSelector\n",
    "\n",
    "each of them with a specific normalizer\n",
    "\n",
    "we also create the OneVsRest for our svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcfa6dc9-def5-45d6-a8ae-a1b523bc5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq = ChiSqSelector(\n",
    "        featuresCol=idf_out, outputCol=\"selected\", labelCol=\"label\", numTopFeatures=2000,\n",
    "    )\n",
    "\n",
    "chi_normalizer = Normalizer(\n",
    "    inputCol=\"selected\", outputCol=\"norm\", p=2.0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a8c75d-1619-40aa-83b1-3bc106910012",
   "metadata": {},
   "outputs": [],
   "source": [
    "vts = VarianceThresholdSelector(            \n",
    "      featuresCol=idf_out, \n",
    "      outputCol=\"selected\"\n",
    "    )\n",
    "\n",
    "vts_normalizer = Normalizer(inputCol=\"selected\", outputCol=\"norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ae0df6-1588-4315-bc94-ea743a6e2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr = OneVsRest(classifier=base_svm,\n",
    "                labelCol=\"label\", featuresCol=\"norm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53afe7a",
   "metadata": {},
   "source": [
    "this creates the two different pipelines for both filtering models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5d56569-1439-43f8-9064-86518457ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_pipeline = Pipeline(stages=stages_no_chi + [chisq, chi_normalizer, ovr])\n",
    "vts_pipeline = Pipeline(stages=stages_no_chi + [vts, vts_normalizer, ovr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcdc5f",
   "metadata": {},
   "source": [
    "here we initialize the ParameterGrid with different parameters for the sv,:\n",
    "\n",
    "- max. Iterations: 5, 10\n",
    "- regularization Parameter: 0.1, 1.0, 10.0\n",
    "- standardization: True, False \n",
    "\n",
    "and create the evaluator with the F1 metric \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aae7a88e-f131-4c80-8478-638f004180ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "        .addGrid(base_svm.maxIter, [5,10])\n",
    "        .addGrid(base_svm.regParam, [0.1, 1.0, 10.0])\n",
    "        .addGrid(base_svm.standardization, [True, False])\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7adfa",
   "metadata": {},
   "source": [
    "here we build a function to run both pipelines and safe the best parameters aswell as the F1 score, \n",
    "we then save the results and print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b85dbc-35e9-4264-8ca1-a7ff9c15dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tvs(pipe, param_grid, name):\n",
    "    tvs = TrainValidationSplit(\n",
    "        estimator=pipe,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        trainRatio=0.8,\n",
    "        seed=SEED\n",
    "    )\n",
    "    print(f\"\\n Fitting TVS for {name} …\")\n",
    "    model = tvs.fit(train_data)\n",
    "    val_metrics = model.validationMetrics\n",
    "    best_i     = val_metrics.index(max(val_metrics))\n",
    "    best_val   = val_metrics[best_i]\n",
    "    best_params= tvs.getEstimatorParamMaps()[best_i]\n",
    "    test_pred  = model.bestModel.transform(test_data)\n",
    "    test_f1    = evaluator.evaluate(test_pred)\n",
    "    print(f\" {name}  Best val F1 = {best_val:.4f}  params={best_params}\")\n",
    "    print(f\" {name}  Test F1     = {test_f1:.4f}\")\n",
    "    return best_val, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "965aaf21-e78d-4550-bb42-9dc1285e9724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fitting TVS for VTS …\n",
      "25/05/13 01:58:33 WARN CacheManager: Asked to cache already cached data.\n",
      "25/05/13 01:58:33 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:00:01 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1800:>                                                       (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:00:02 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:01:51 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2502:>                                                       (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:01:58 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:03:44 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3258:>                                                       (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:03:55 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:05:47 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:06:21 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:07:30 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:09:12 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:09:44 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:11:23 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:11:55 WARN BlockManager: Asked to remove block broadcast_14021, which does not exist\n",
      "25/05/13 02:11:57 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:14:13 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8353:>                                                       (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:14:20 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:16:08 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:16:34 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:17:12 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:19:25 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:20:18 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:21:05 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:21:53 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:24:35 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:26:49 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VTS  Best val F1 = 0.4068  params={Param(parent='LinearSVC_6d0c326237d8', name='maxIter', doc='max number of iterations (>= 0).'): 5, Param(parent='LinearSVC_6d0c326237d8', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='LinearSVC_6d0c326237d8', name='standardization', doc='whether to standardize the training features before fitting the model.'): False}\n",
      " VTS  Test F1     = 0.4202\n",
      "\n",
      " Fitting TVS for ChiSq …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:29:05 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:31:19 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:33:27 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:35:45 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:37:48 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:40:15 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:42:50 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:45:48 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:48:28 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:51:51 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:54:48 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 02:58:29 WARN DAGScheduler: Broadcasting large task binary with size 1235.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 03:00:57 WARN DAGScheduler: Broadcasting large task binary with size 1233.6 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21002:===========================>                           (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ChiSq  Best val F1 = 0.3612  params={Param(parent='LinearSVC_6d0c326237d8', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LinearSVC_6d0c326237d8', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='LinearSVC_6d0c326237d8', name='standardization', doc='whether to standardize the training features before fitting the model.'): True}\n",
      " ChiSq  Test F1     = 0.3814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "results['var'], results['var_test'] = run_tvs(vts_pipeline, param_grid, \"VTS\")\n",
    "results['chi2'], results['chi2_test'] = run_tvs(chi_pipeline, param_grid, \"ChiSq\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c94af-6c2c-49a2-92c2-0cc21c9b39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
