{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f75e6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jonas\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jonas\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jonas\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jonas\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark matplotlib pandas numpy scikit-learn\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, ChiSqSelector, Normalizer, StringIndexer\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set a fixed random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd740d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:329)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize SparkSession\u001b[39;00m\n\u001b[0;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDIC EX 2 - group 36\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark session initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:329)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\r\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DIC EX 2 - group 36\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408639b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the development dataset\n",
    "print(\"Loading review data...\")\n",
    "data_path = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "df = spark.read.json(data_path)\n",
    "\n",
    "# Display schema and sample data\n",
    "print(\"Dataset Schema:\")\n",
    "df.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "df.select(\"reviewText\", \"category\").show(5, truncate=True)\n",
    "\n",
    "# Function to check dataset statistics\n",
    "def print_stats(df):\n",
    "    total_reviews = df.count()\n",
    "    category_counts = df.groupBy(\"category\").count().orderBy(\"count\", ascending=False)\n",
    "    \n",
    "    print(f\"Total number of reviews: {total_reviews}\")\n",
    "    print(\"Category distribution:\")\n",
    "    category_counts.show(20, truncate=False)\n",
    "    \n",
    "    return category_counts\n",
    "\n",
    "# Get dataset statistics\n",
    "category_counts = print_stats(df)\n",
    "\n",
    "# Convert category_counts to Pandas for visualization\n",
    "category_dist = category_counts.toPandas()\n",
    "top_10_categories = category_dist.head(10)\n",
    "\n",
    "# Visualization of category distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_10_categories['category'], top_10_categories['count'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Distribution of Top 10 Review Categories')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('category_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Display the plot\n",
    "from IPython.display import Image\n",
    "Image(filename='category_distribution.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation: Split into training, validation, and test sets\n",
    "print(\"Splitting data into training, validation, and test sets...\")\n",
    "train_data, temp_data = df.randomSplit([0.7, 0.3], seed=seed)\n",
    "validation_data, test_data = temp_data.randomSplit([0.5, 0.5], seed=seed)\n",
    "\n",
    "print(f\"Training set size: {train_data.count()}\")\n",
    "print(f\"Validation set size: {validation_data.count()}\")\n",
    "print(f\"Test set size: {test_data.count()}\")\n",
    "\n",
    "# Load the top features selected in Part 2\n",
    "print(\"Loading selected features from output_ds.txt...\")\n",
    "with open(\"output_ds.txt\", \"r\") as f:\n",
    "    selected_features = f.read().strip().split()\n",
    "\n",
    "print(f\"Loaded {len(selected_features)} features from output_ds.txt\")\n",
    "print(f\"Sample features: {selected_features[:10]}\")\n",
    "\n",
    "print(\"Building the ML Pipeline for classification...\")\n",
    "\n",
    "# Create a pipeline similar to Part 2 but configured for classification\n",
    "# 1. Convert category to numeric labels\n",
    "label_indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "\n",
    "# 2. Text preprocessing (similar to Part 2)\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"tokens\", \n",
    "                          pattern=\"[\\\\s\\\\t\\\\d\\\\(\\\\)\\\\[\\\\]\\\\{\\\\}\\\\.\\\\!\\\\?\\\\,\\\\;\\\\:\\\\+\\\\=\\\\-\\\\_\\\\\\\"\\\\'`\\\\~\\\\#\\\\@\\\\&\\\\*\\\\%\\\\€\\\\$\\\\§\\\\\\\\\\\\/]+\")\n",
    "\n",
    "# 3. Load stopwords\n",
    "def load_stopwords(path: str) -> list[str]:\n",
    "    stopwords = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        stopwords = set(line.strip() for line in f if line.strip())\n",
    "    return list(stopwords)\n",
    "\n",
    "# 4. Remove stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_filtered\", \n",
    "                                    stopWords=load_stopwords(\"stopwords.txt\"))\n",
    "\n",
    "# 5. Create term frequency vectors\n",
    "count_vectorizer = CountVectorizer(inputCol=\"tokens_filtered\", outputCol=\"tf\")\n",
    "\n",
    "# 6. Calculate IDF\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tf_idf\")\n",
    "\n",
    "# 7. Feature selection using Chi-Square\n",
    "# Here we'll use two different feature dimensionalities for comparison\n",
    "# 7.1 - 2000 features (as required in the assignment)\n",
    "chi_sq_selector_2000 = ChiSqSelector(numTopFeatures=2000, \n",
    "                                    featuresCol=\"tf_idf\", \n",
    "                                    outputCol=\"selected_features\",\n",
    "                                    labelCol=\"label\")\n",
    "\n",
    "# 7.2 - A much smaller dimensionality for comparison (500 features)\n",
    "chi_sq_selector_500 = ChiSqSelector(numTopFeatures=500, \n",
    "                                   featuresCol=\"tf_idf\", \n",
    "                                   outputCol=\"selected_features\",\n",
    "                                   labelCol=\"label\")\n",
    "\n",
    "# 8. Vector normalization (required by the assignment)\n",
    "normalizer = Normalizer(inputCol=\"selected_features\", outputCol=\"normalized_features\", p=2.0)\n",
    "\n",
    "# 9. Define the SVM classifier\n",
    "svm = LinearSVC(featuresCol=\"normalized_features\", labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Build the pipeline with 2000 features\n",
    "pipeline_2000 = Pipeline(stages=[\n",
    "    label_indexer,\n",
    "    tokenizer,\n",
    "    stopwords_remover,\n",
    "    count_vectorizer,\n",
    "    idf,\n",
    "    chi_sq_selector_2000,\n",
    "    normalizer,\n",
    "    svm\n",
    "])\n",
    "\n",
    "# Build the pipeline with 500 features\n",
    "pipeline_500 = Pipeline(stages=[\n",
    "    label_indexer,\n",
    "    tokenizer,\n",
    "    stopwords_remover,\n",
    "    count_vectorizer,\n",
    "    idf,\n",
    "    chi_sq_selector_500,\n",
    "    normalizer,\n",
    "    svm\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad733532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid for the SVM\n",
    "print(\"Setting up parameter grid for SVM optimization...\")\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(svm.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(svm.standardization, [True, False]) \\\n",
    "    .addGrid(svm.maxIter, [10, 50]) \\\n",
    "    .build()\n",
    "\n",
    "# Create an evaluator for model assessment\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "# Create cross-validators for both feature dimensionalities\n",
    "cv_2000 = CrossValidator(\n",
    "    estimator=pipeline_2000,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "cv_500 = CrossValidator(\n",
    "    estimator=pipeline_500,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Train and evaluate models\n",
    "results = []\n",
    "\n",
    "# Function to extract parameter settings from a model\n",
    "def extract_params(pipeline_model):\n",
    "    svm_model = None\n",
    "    for stage in pipeline_model.stages:\n",
    "        if isinstance(stage, type(LinearSVC())):\n",
    "            svm_model = stage\n",
    "            break\n",
    "    \n",
    "    if svm_model:\n",
    "        return {\n",
    "            \"regParam\": svm_model.getRegParam(),\n",
    "            \"standardization\": svm_model.getStandardization(),\n",
    "            \"maxIter\": svm_model.getMaxIter()\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# Train model with 2000 features\n",
    "print(\"\\nTraining model with 2000 features...\")\n",
    "print(\"This may take some time...\")\n",
    "cv_model_2000 = cv_2000.fit(train_data)\n",
    "\n",
    "# Get the best model\n",
    "best_model_2000 = cv_model_2000.bestModel\n",
    "params_2000 = extract_params(best_model_2000)\n",
    "\n",
    "# Apply the best model to the test set\n",
    "predictions_2000 = best_model_2000.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "f1_score_2000 = evaluator.evaluate(predictions_2000)\n",
    "\n",
    "# Store results\n",
    "results.append({\n",
    "    \"feature_set\": \"2000 features\",\n",
    "    \"regParam\": params_2000[\"regParam\"],\n",
    "    \"standardization\": params_2000[\"standardization\"],\n",
    "    \"maxIter\": params_2000[\"maxIter\"],\n",
    "    \"f1_score\": f1_score_2000\n",
    "})\n",
    "\n",
    "print(f\"\\nBest parameters for 2000 features:\")\n",
    "print(f\"  regParam: {params_2000['regParam']}\")\n",
    "print(f\"  standardization: {params_2000['standardization']}\")\n",
    "print(f\"  maxIter: {params_2000['maxIter']}\")\n",
    "print(f\"  F1 score on test set: {f1_score_2000:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac63193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with 500 features\n",
    "print(\"\\nTraining model with 500 features...\")\n",
    "print(\"This may take some time...\")\n",
    "cv_model_500 = cv_500.fit(train_data)\n",
    "\n",
    "# Get the best model\n",
    "best_model_500 = cv_model_500.bestModel\n",
    "params_500 = extract_params(best_model_500)\n",
    "\n",
    "# Apply the best model to the test set\n",
    "predictions_500 = best_model_500.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "f1_score_500 = evaluator.evaluate(predictions_500)\n",
    "\n",
    "# Store results\n",
    "results.append({\n",
    "    \"feature_set\": \"500 features\",\n",
    "    \"regParam\": params_500[\"regParam\"],\n",
    "    \"standardization\": params_500[\"standardization\"],\n",
    "    \"maxIter\": params_500[\"maxIter\"],\n",
    "    \"f1_score\": f1_score_500\n",
    "})\n",
    "\n",
    "print(f\"\\nBest parameters for 500 features:\")\n",
    "print(f\"  regParam: {params_500['regParam']}\")\n",
    "print(f\"  standardization: {params_500['standardization']}\")\n",
    "print(f\"  maxIter: {params_500['maxIter']}\")\n",
    "print(f\"  F1 score on test set: {f1_score_500:.4f}\")\n",
    "\n",
    "# Save confusion matrix for the best model\n",
    "# Find which model performed better\n",
    "best_predictions = predictions_2000 if f1_score_2000 > f1_score_500 else predictions_500\n",
    "best_feature_set = \"2000 features\" if f1_score_2000 > f1_score_500 else \"500 features\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get label mapping from the indexer\n",
    "label_mapping = {idx: cat for idx, cat in enumerate(label_indexer.fit(df).labels)}\n",
    "\n",
    "# Convert predictions to a pandas DataFrame for easier analysis\n",
    "pred_df = best_predictions.select(\"category\", \"prediction\", \"label\").toPandas()\n",
    "true_labels = pred_df[\"label\"]\n",
    "pred_labels = pred_df[\"prediction\"]\n",
    "\n",
    "# Calculate confusion matrix for top categories\n",
    "top_categories = category_counts.limit(10).toPandas()['category'].tolist()\n",
    "category_to_idx = {cat: idx for idx, cat in enumerate(label_indexer.fit(df).labels)}\n",
    "top_indices = [category_to_idx[cat] for cat in top_categories]\n",
    "\n",
    "# Initialize confusion matrix\n",
    "conf_matrix = np.zeros((len(top_indices), len(top_indices)), dtype=int)\n",
    "\n",
    "# Fill confusion matrix\n",
    "for i, true_cat in enumerate(top_indices):\n",
    "    for j, pred_cat in enumerate(top_indices):\n",
    "        conf_matrix[i, j] = sum((true_labels == true_cat) & (pred_labels == pred_cat))\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(f'Confusion Matrix - Top 10 Categories ({best_feature_set})')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(top_categories))\n",
    "plt.xticks(tick_marks, top_categories, rotation=90)\n",
    "plt.yticks(tick_marks, top_categories)\n",
    "plt.ylabel('True Category')\n",
    "plt.xlabel('Predicted Category')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'confusion_matrix_{best_feature_set.replace(\" \", \"_\")}.png')\n",
    "plt.close()\n",
    "\n",
    "# Display the plot\n",
    "Image(filename=f'confusion_matrix_{best_feature_set.replace(\" \", \"_\")}.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bca5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize results in a table\n",
    "print(\"\\nSummary of Model Performance:\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Visualization of results\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(results_df['feature_set'], results_df['f1_score'])\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.title('F1 Score Comparison: 2000 vs 500 Features')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xlabel('Feature Set')\n",
    "plt.ylim(0, max(results_df['f1_score']) * 1.1)  # Add 10% padding at the top\n",
    "plt.savefig('f1_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Display the plot\n",
    "Image(filename='f1_comparison.png')\n",
    "\n",
    "# Analyze the effect of different parameters\n",
    "# Extract metrics from cross-validation results for 2000 features\n",
    "cv_metrics_2000 = []\n",
    "for i, metric in enumerate(cv_model_2000.avgMetrics):\n",
    "    params = param_grid[i]\n",
    "    reg_param = params[svm.regParam]\n",
    "    standardization = params[svm.standardization]\n",
    "    max_iter = params[svm.maxIter]\n",
    "    \n",
    "    cv_metrics_2000.append({\n",
    "        'regParam': reg_param,\n",
    "        'standardization': standardization,\n",
    "        'maxIter': max_iter,\n",
    "        'f1_score': metric,\n",
    "        'feature_set': '2000 features'\n",
    "    })\n",
    "\n",
    "# Extract metrics from cross-validation results for 500 features\n",
    "cv_metrics_500 = []\n",
    "for i, metric in enumerate(cv_model_500.avgMetrics):\n",
    "    params = param_grid[i]\n",
    "    reg_param = params[svm.regParam]\n",
    "    standardization = params[svm.standardization]\n",
    "    max_iter = params[svm.maxIter]\n",
    "    \n",
    "    cv_metrics_500.append({\n",
    "        'regParam': reg_param,\n",
    "        'standardization': standardization,\n",
    "        'maxIter': max_iter,\n",
    "        'f1_score': metric,\n",
    "        'feature_set': '500 features'\n",
    "    })\n",
    "\n",
    "# Combine metrics\n",
    "all_metrics = cv_metrics_2000 + cv_metrics_500\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "# Analyze effect of regularization parameter\n",
    "plt.figure(figsize=(12, 6))\n",
    "for feature_set in ['2000 features', '500 features']:\n",
    "    subset = metrics_df[metrics_df['feature_set'] == feature_set]\n",
    "    reg_effect = subset.groupby('regParam')['f1_score'].mean()\n",
    "    plt.plot(reg_effect.index, reg_effect.values, marker='o', label=feature_set)\n",
    "\n",
    "plt.title('Effect of Regularization Parameter on F1 Score')\n",
    "plt.xlabel('Regularization Parameter (regParam)')\n",
    "plt.ylabel('Average F1 Score')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('regparam_effect.png')\n",
    "plt.close()\n",
    "\n",
    "# Display the plot\n",
    "Image(filename='regparam_effect.png')\n",
    "\n",
    "# Analyze effect of standardization\n",
    "plt.figure(figsize=(10, 6))\n",
    "std_effect = metrics_df.groupby(['feature_set', 'standardization'])['f1_score'].mean().reset_index()\n",
    "\n",
    "# Plot for 2000 features\n",
    "std_effect_2000 = std_effect[std_effect['feature_set'] == '2000 features']\n",
    "plt.bar([0, 2], std_effect_2000['f1_score'], width=0.4, label='2000 features')\n",
    "\n",
    "# Plot for 500 features\n",
    "std_effect_500 = std_effect[std_effect['feature_set'] == '500 features']\n",
    "plt.bar([0.5, 2.5], std_effect_500['f1_score'], width=0.4, label='500 features')\n",
    "\n",
    "plt.title('Effect of Standardization on F1 Score')\n",
    "plt.xlabel('Standardization')\n",
    "plt.xticks([0.25, 2.25], ['False', 'True'])\n",
    "plt.ylabel('Average F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "plt.savefig('standardization_effect.png')\n",
    "plt.close()\n",
    "\n",
    "# Display the plot\n",
    "Image(filename='standardization_effect.png')\n",
    "\n",
    "# Analyze effect of maximum iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "iter_effect = metrics_df.groupby(['feature_set', 'maxIter'])['f1_score'].mean().reset_index()\n",
    "\n",
    "# Plot for 2000 features\n",
    "iter_effect_2000 = iter_effect[iter_effect['feature_set'] == '2000 features']\n",
    "plt.bar([0, 2], iter_effect_2000['f1_score'], width=0.4, label='2000 features')\n",
    "\n",
    "# Plot for 500 features\n",
    "iter_effect_500 = iter_effect[iter_effect['feature_set'] == '500 features']\n",
    "plt.bar([0.5, 2.5], iter_effect_500['f1_score'], width=0.4, label='500 features')\n",
    "\n",
    "plt.title('Effect of Maximum Iterations on F1 Score')\n",
    "plt.xlabel('Maximum Iterations')\n",
    "plt.xticks([0.25, 2.25], ['10', '50'])\n",
    "plt.ylabel('Average F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "plt.savefig('maxiter_effect.png')\n",
    "plt.close()\n",
    "\n",
    "# Display the plot\n",
    "Image(filename='maxiter_effect.png')\n",
    "\n",
    "# Determine the overall best model configuration\n",
    "best_config = max(results, key=lambda x: x['f1_score'])\n",
    "print(\"\\nBest Overall Configuration:\")\n",
    "print(f\"Feature Set: {best_config['feature_set']}\")\n",
    "print(f\"regParam: {best_config['regParam']}\")\n",
    "print(f\"standardization: {best_config['standardization']}\")\n",
    "print(f\"maxIter: {best_config['maxIter']}\")\n",
    "print(f\"F1 Score: {best_config['f1_score']:.4f}\")\n",
    "\n",
    "# Save the best model parameters\n",
    "with open(\"best_model_params.txt\", \"w\") as f:\n",
    "    f.write(f\"Feature Set: {best_config['feature_set']}\\n\")\n",
    "    f.write(f\"regParam: {best_config['regParam']}\\n\")\n",
    "    f.write(f\"standardization: {best_config['standardization']}\\n\")\n",
    "    f.write(f\"maxIter: {best_config['maxIter']}\\n\")\n",
    "    f.write(f\"F1 Score: {best_config['f1_score']:.4f}\\n\")\n",
    "\n",
    "print(\"\\nBest model parameters saved to 'best_model_params.txt'\")\n",
    "\n",
    "# Conclusion\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"We have successfully implemented a text classification pipeline using Spark ML.\")\n",
    "print(\"The pipeline includes text preprocessing, feature extraction with TF-IDF, and SVM classification.\")\n",
    "print(\"We compared two feature dimensions (2000 vs 500 features) and varied SVM parameters.\")\n",
    "print(f\"The best model achieved an F1 score of {best_config['f1_score']:.4f} on the test set.\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
