{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875e7f04-c3e5-44d7-b604-9259bb131bd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# DIC EX2 - part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c9941-f61e-48f4-a5c3-01b2de8d45f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "### Initialize Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2590cf54-9939-4f23-be41-abff87d7ce14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4054. Attempting port 4055.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4055. Attempting port 4056.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4056. Attempting port 4057.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4057. Attempting port 4058.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4058. Attempting port 4059.\n",
      "25/05/06 14:40:13 WARN Utils: Service 'SparkUI' could not bind on port 4059. Attempting port 4060.\n",
      "25/05/06 14:40:15 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DIC EX 2 - group 36\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b6529-e715-4393-97be-3f36d9e254cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Set path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efdf2c4c-cd22-429d-9937-5bf54249bc46",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "stopwords_path = \"stopwords.txt\"\n",
    "output_path = \"output_ds.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de0bc7-3f76-48eb-865a-0889b004c1ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402c5577-b0fd-47da-86d8-d87dd13afdde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(data_path)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a4676-7bde-4402-892d-dbf004e7cba5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Build pipeline\n",
    "\n",
    "### Tokenize using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2597d10b-c173-42b9-af39-3b0a640b0073",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          reviewText|              tokens|\n",
      "+--------------------+--------------------+\n",
      "|This was a gift f...|[this, was, a, gi...|\n",
      "|This is a very ni...|[this, is, a, ver...|\n",
      "|The metal base wi...|[the, metal, base...|\n",
      "|For the most part...|[for, the, most, ...|\n",
      "|This hose is supp...|[this, hose, is, ...|\n",
      "|This tool works v...|[this, tool, work...|\n",
      "|This product is a...|[this, product, i...|\n",
      "|I was excited to ...|[i, was, excited,...|\n",
      "|I purchased the L...|[i, purchased, th...|\n",
      "|Never used a manu...|[never, used, a, ...|\n",
      "|Good price. Good ...|[good, price, goo...|\n",
      "|I have owned the ...|[i, have, owned, ...|\n",
      "|I had \"won\" a sim...|[i, had, won, a, ...|\n",
      "|The birds ate all...|[the, birds, ate,...|\n",
      "|Bought last summe...|[bought, last, su...|\n",
      "|I knew I had a mo...|[i, knew, i, had,...|\n",
      "|I was a little wo...|[i, was, a, littl...|\n",
      "|I have used this ...|[i, have, used, t...|\n",
      "|I actually do not...|[i, actually, do,...|\n",
      "|Just what I  expe...|[just, what, i, e...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"tokens\", pattern=\"[\\s\\t\\d\\(\\)\\[\\]\\{\\}\\.\\!\\?\\,\\;\\:\\+\\=\\-\\_\\\"\\'`\\~\\#\\@\\&\\*\\%\\€\\$\\§\\\\\\/]+\")\n",
    "tokenized = tokenizer.transform(df)\n",
    "tokenized.select(\"reviewText\", \"tokens\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4540de42-6fa2-4287-91fc-f2cea5bdaf85",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dc3f90d-6e83-414a-ab32-bb4c5cbff71e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              tokens|     tokens_filtered|\n",
      "+--------------------+--------------------+\n",
      "|[this, was, a, gi...|[gift, husband, m...|\n",
      "|[this, is, a, ver...|[nice, spreader, ...|\n",
      "|[the, metal, base...|[metal, base, hos...|\n",
      "|[for, the, most, ...|[part, works, pre...|\n",
      "|[this, hose, is, ...|[hose, supposed, ...|\n",
      "|[this, tool, work...|[tool, works, cut...|\n",
      "|[this, product, i...|[typical, usable,...|\n",
      "|[i, was, excited,...|[excited, ditch, ...|\n",
      "|[i, purchased, th...|[purchased, leaf,...|\n",
      "|[never, used, a, ...|[manual, lawnmowe...|\n",
      "|[good, price, goo...|[good, price, goo...|\n",
      "|[i, have, owned, ...|[owned, flowtron,...|\n",
      "|[i, had, won, a, ...|[similar, family,...|\n",
      "|[the, birds, ate,...|[birds, ate, blue...|\n",
      "|[bought, last, su...|[bought, summer, ...|\n",
      "|[i, knew, i, had,...|[knew, mouse, bas...|\n",
      "|[i, was, a, littl...|[worried, reading...|\n",
      "|[i, have, used, t...|[brand, long, tim...|\n",
      "|[i, actually, do,...|[current, model, ...|\n",
      "|[just, what, i, e...|[expected, works,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "def load_stopwords(path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Load stopwords from a file efficiently.\n",
    "    \"\"\"\n",
    "    stopwords = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        stopwords = set(line.strip() for line in f if line.strip())\n",
    "    return list(stopwords)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_filtered\", stopWords=load_stopwords(stopwords_path))\n",
    "removed = remover.transform(tokenized)\n",
    "removed.select(\"tokens\", \"tokens_filtered\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb7700-3065-4bc5-bc77-8e26b14ae205",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Calculate token counts and idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0fefb5d-a405-412a-a01e-114e9a45494c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/06 14:43:16 WARN DAGScheduler: Broadcasting large task binary with size 1063.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/06 14:43:27 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "+--------------------+--------------------+--------------------+\n",
      "|     tokens_filtered|        token_counts|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[gift, husband, m...|(96130,[2,3,7,8,3...|(96130,[2,3,7,8,3...|\n",
      "|[nice, spreader, ...|(96130,[0,1,3,21,...|(96130,[0,1,3,21,...|\n",
      "|[metal, base, hos...|(96130,[4,10,29,1...|(96130,[4,10,29,1...|\n",
      "|[part, works, pre...|(96130,[1,3,4,9,1...|(96130,[1,3,4,9,1...|\n",
      "|[hose, supposed, ...|(96130,[12,32,42,...|(96130,[12,32,42,...|\n",
      "|[tool, works, cut...|(96130,[0,3,4,8,1...|(96130,[0,3,4,8,1...|\n",
      "|[typical, usable,...|(96130,[18,63,122...|(96130,[18,63,122...|\n",
      "|[excited, ditch, ...|(96130,[6,21,35,3...|(96130,[6,21,35,3...|\n",
      "|[purchased, leaf,...|(96130,[3,4,5,6,4...|(96130,[3,4,5,6,4...|\n",
      "|[manual, lawnmowe...|(96130,[6,8,41,87...|(96130,[6,8,41,87...|\n",
      "|[good, price, goo...|(96130,[1,13,95,2...|(96130,[1,13,95,2...|\n",
      "|[owned, flowtron,...|(96130,[5,17,36,4...|(96130,[5,17,36,4...|\n",
      "|[similar, family,...|(96130,[1,11,31,3...|(96130,[1,11,31,3...|\n",
      "|[birds, ate, blue...|(96130,[44,160,28...|(96130,[44,160,28...|\n",
      "|[bought, summer, ...|(96130,[0,3,7,9,1...|(96130,[0,3,7,9,1...|\n",
      "|[knew, mouse, bas...|(96130,[8,28,29,6...|(96130,[8,28,29,6...|\n",
      "|[worried, reading...|(96130,[1,15,130,...|(96130,[1,15,130,...|\n",
      "|[brand, long, tim...|(96130,[2,3,23,25...|(96130,[2,3,23,25...|\n",
      "|[current, model, ...|(96130,[4,10,16,2...|(96130,[4,10,16,2...|\n",
      "|[expected, works,...|(96130,[0,18,33,4...|(96130,[0,18,33,4...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer\n",
    "\n",
    "countModel = CountVectorizer(inputCol=\"tokens_filtered\", outputCol=\"token_counts\").fit(removed)\n",
    "featurizedData = countModel.transform(removed)\n",
    "\n",
    "idfModel = IDF(inputCol=\"token_counts\", outputCol=\"features\").fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"tokens_filtered\", \"token_counts\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc385e8-e666-42d5-bb04-0478ba481ba0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Calculate chi square values and select top 75 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b77c6d5-b5a3-4172-882b-1fbb11ea3839",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/06 14:43:31 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/06 14:43:33 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/06 14:43:42 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/06 14:44:04 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|            category|          reviewText|     tokens_filtered|    selectedFeatures|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...|[gift, husband, m...|(75,[2,3,5,30],[5...|\n",
      "|Patio_Lawn_and_Garde|This is a very ni...|[nice, spreader, ...|(75,[0,1,3,18,65]...|\n",
      "|Patio_Lawn_and_Garde|The metal base wi...|[metal, base, hos...|(75,[4,7],[2.4430...|\n",
      "|Patio_Lawn_and_Garde|For the most part...|[part, works, pre...|(75,[1,3,4,6,7,15...|\n",
      "|Patio_Lawn_and_Garde|This hose is supp...|[hose, supposed, ...|(75,[9],[2.627002...|\n",
      "|Patio_Lawn_and_Garde|This tool works v...|[tool, works, cut...|(75,[0,3,4,8,15,3...|\n",
      "|Patio_Lawn_and_Garde|This product is a...|[typical, usable,...|(75,[15],[8.09593...|\n",
      "|Patio_Lawn_and_Garde|I was excited to ...|[excited, ditch, ...|(75,[18,27,31,49]...|\n",
      "|Patio_Lawn_and_Garde|I purchased the L...|[purchased, leaf,...|(75,[3,4,34,46],[...|\n",
      "|Patio_Lawn_and_Garde|Never used a manu...|[manual, lawnmowe...|(75,[33,63],[3.08...|\n",
      "|Patio_Lawn_and_Garde|Good price. Good ...|[good, price, goo...|(75,[1,10,70],[3....|\n",
      "|Patio_Lawn_and_Garde|I have owned the ...|[owned, flowtron,...|(75,[14,28,34,37,...|\n",
      "|Patio_Lawn_and_Garde|I had \"won\" a sim...|[similar, family,...|(75,[1,8,30,41,47...|\n",
      "|Patio_Lawn_and_Garde|The birds ate all...|[birds, ate, blue...|(75,[34],[6.34856...|\n",
      "|Patio_Lawn_and_Garde|Bought last summe...|[bought, summer, ...|(75,[0,3,5,6,8,24...|\n",
      "|Patio_Lawn_and_Garde|I knew I had a mo...|[knew, mouse, bas...|(75,[23,47,65,68]...|\n",
      "|Patio_Lawn_and_Garde|I was a little wo...|[worried, reading...|(75,[1,12],[1.593...|\n",
      "|Patio_Lawn_and_Garde|I have used this ...|[brand, long, tim...|(75,[2,3],[1.8826...|\n",
      "|Patio_Lawn_and_Garde|I actually do not...|[current, model, ...|(75,[4,7,13,17,18...|\n",
      "|Patio_Lawn_and_Garde|Just what I  expe...|[expected, works,...|(75,[0,15,25,36],...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "labels = {}\n",
    "\n",
    "def label_fn(x: str) -> int:\n",
    "    if x not in labels:\n",
    "        labels[x] = len(labels)\n",
    "    return labels[x]\n",
    "\n",
    "udfCategoryToLabel = udf(label_fn, IntegerType())\n",
    "labeled = rescaledData.withColumn(\"label\", udfCategoryToLabel(\"category\"))\n",
    "\n",
    "result = ChiSqSelector(numTopFeatures=75, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"label\").fit(labeled).transform(labeled)\n",
    "\n",
    "result.select(\"category\", \"reviewText\", \"tokens_filtered\", \"selectedFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51250d-8ccb-4edb-b229-3d894403371d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Get top tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6193ed-fd8c-4685-918d-fe69ebcca7df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/06 14:44:05 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon', 'author', 'back', 'bad', 'big', 'bit', 'bought', 'buy', 'character', 'characters', 'day', 'easy', 'end', 'enjoyed', 'excellent', 'family', 'feel', 'find', 'fit', 'found', 'give', 'good', 'great', 'happy', 'hard', 'high', 'highly', 'interesting', 'job', 'light', 'long', 'lot', 'love', 'loved', 'made', 'make', 'makes', 'man', 'money', 'music', 'nice', 'part', 'people', 'perfect', 'pretty', 'price', 'problem', 'purchase', 'purchased', 'put', 'quality', 'quot', 'reading', 'real', 'recommend', 'review', 'series', 'set', 'size', 'small', 'sound', 'thing', 'things', 'thought', 'time', 'times', 'wanted', 'watch', 'work', 'works', 'world', 'worth', 'written', 'year', 'years']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "def extract_indices(sparse_vector):\n",
    "    return sparse_vector.indices.tolist()\n",
    "\n",
    "extract_indices_udf = udf(extract_indices, ArrayType(IntegerType()))\n",
    "\n",
    "df_with_indices = result.withColumn(\"indices\", extract_indices_udf(result[\"selectedFeatures\"]))\n",
    "indices = df_with_indices.select(\"indices\").rdd.flatMap(lambda row: row.indices).distinct().collect()\n",
    "\n",
    "vocab = countModel.vocabulary\n",
    "words = [vocab[index] for index in indices]\n",
    "print(sorted(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4eae1-a351-4faa-8a88-76c28e2db169",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Write tokens to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76c07d98-482c-4a8d-ab97-6ddc69084795",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(output_path, \"w\") as f:\n",
    "    f.write(\" \".join(sorted(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df8e79-3270-4063-b293-f1f55891c3b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
